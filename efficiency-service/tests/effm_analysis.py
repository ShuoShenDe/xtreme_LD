#!/usr/bin/env python3
"""
EFFMæ•ˆç‡ç›‘æ§äº‹ä»¶ä¸“ç”¨åˆ†æè„šæœ¬
åŠŸèƒ½ï¼š
1. æ¯ç§æ ‡æ³¨ç±»å‹çš„æ€»æ•°é‡ï¼ˆæ€»ä½“å’Œåˆ†ç”¨æˆ·ï¼‰
2. å®Œæˆä¸€ä¸ªç±»å‹çš„æ ‡æ³¨éœ€è¦çš„æ—¶é—´
3. ç”¨æˆ·æ²¡æœ‰æ“ä½œçš„idleæ—¶é—´åˆ†æ
"""

import sys
import json
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import argparse

try:
    from influxdb_client.client.influxdb_client import InfluxDBClient
    from influxdb_client.client.query_api import QueryApi
except ImportError:
    print("âŒ ç¼ºå°‘ influxdb-client ä¾èµ–")
    print("è¯·å®‰è£…: pip install influxdb-client pandas")
    sys.exit(1)

# InfluxDB é…ç½®
INFLUXDB_CONFIG = {
    'url': 'http://localhost:8087',
    'token': 'Y7anaf-f1yBZaDe3M1pCy5LdfVTxH8g8odTzf0UOJd_0V4BROJmQ7HlFDTLefh8GIoWNNkOgKHdnKAeR7KMhqw==',
    'org': 'xtreme1',
    'bucket': 'efficiency_events'
}

class EffmAnalyzer:
    def __init__(self):
        """åˆå§‹åŒ– EFFM åˆ†æå™¨"""
        self.client = None
        self.query_api = None
        
    def connect(self) -> bool:
        """è¿æ¥åˆ° InfluxDB"""
        try:
            self.client = InfluxDBClient(
                url=INFLUXDB_CONFIG['url'],
                token=INFLUXDB_CONFIG['token'],
                org=INFLUXDB_CONFIG['org']
            )
            self.query_api = self.client.query_api()
            
            # æµ‹è¯•è¿æ¥
            health = self.client.health()
            if health.status == "pass":
                print("âœ… InfluxDB è¿æ¥æˆåŠŸ")
                return True
            else:
                print(f"âŒ InfluxDB å¥åº·æ£€æŸ¥å¤±è´¥: {health.status}")
                return False
                
        except Exception as e:
            print(f"âŒ InfluxDB è¿æ¥å¤±è´¥: {e}")
            return False
    
    def query_effm_data(self, hours: int = 1) -> Optional[List[Dict]]:
        """æŸ¥è¯¢EFFMäº‹ä»¶æ•°æ®"""
        try:
            query = f'''
            from(bucket: "{INFLUXDB_CONFIG['bucket']}")
              |> range(start: -{hours}h)
              |> filter(fn: (r) => r["_measurement"] == "efficiency_events")
              |> sort(columns: ["_time"], desc: false)
            '''
            
            print(f"ğŸ” æ­£åœ¨æŸ¥è¯¢æœ€è¿‘ {hours} å°æ—¶çš„EFFMæ•°æ®...")
            
            if not self.query_api:
                raise Exception("Query API not initialized")
            
            result = self.query_api.query(org=INFLUXDB_CONFIG['org'], query=query)
            
            records = []
            for table in result:
                for record in table.records:
                    records.append({
                        'time': record.get_time(),
                        'measurement': record.get_measurement(),
                        'field': record.get_field(),
                        'value': record.get_value(),
                        **record.values
                    })
            
            print(f"âœ… è·å–åˆ° {len(records)} æ¡EFFMè®°å½•")
            return records
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢EFFMæ•°æ®å¤±è´¥: {e}")
            return None
    
    def analyze_annotation_statistics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æ1: æ¯ç§æ ‡æ³¨ç±»å‹çš„æ€»æ•°é‡ï¼ˆæ€»ä½“å’Œåˆ†ç”¨æˆ·ï¼‰"""
        if df.empty:
            return {"error": "æ²¡æœ‰æ•°æ®å¯åˆ†æ"}
        
        # å¯»æ‰¾æ ‡æ³¨ç±»å‹å­—æ®µ
        type_fields = ['annotationType', 'annotation_type', 'toolType', 'tool_type', 'field']
        type_col = None
        
        for field in type_fields:
            if field in df.columns:
                type_col = field
                break
        
        if not type_col:
            return {"error": "æœªæ‰¾åˆ°æ ‡æ³¨ç±»å‹ä¿¡æ¯"}
        
        # å¯»æ‰¾ç”¨æˆ·å­—æ®µ
        user_fields = ['user_id', 'userId', 'createdBy', 'created_by']
        user_col = None
        for field in user_fields:
            if field in df.columns:
                user_col = field
                break
        
        # è¿‡æ»¤å®Œæˆçš„æ ‡æ³¨äº‹ä»¶
        completion_mask = (
            (df.get('action', pd.Series(dtype='object')).fillna('') == 'complete') |
            (df.get('event_type', pd.Series(dtype='object')).fillna('').str.contains('completion|completed', case=False, na=False)) |
            (df.get('field', pd.Series(dtype='object')).fillna('').str.contains('completion|completed', case=False, na=False))
        )
        
        completion_df = df[completion_mask]
        
        if completion_df.empty:
            completion_df = df  # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„å®Œæˆæ ‡è¯†ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®
        
        # æ€»ä½“ç»Ÿè®¡
        if type_col in completion_df.columns:
            total_by_type = completion_df[type_col].value_counts()
        else:
            return {"error": f"å­—æ®µ {type_col} ä¸å­˜åœ¨"}
        
        result = {
            "total_annotations": len(completion_df),
            "annotation_types": len(total_by_type),
            "overall_statistics": {
                "by_type": total_by_type.to_dict(),
                "type_percentages": (total_by_type / len(completion_df) * 100).to_dict()
            }
        }
        
        # åˆ†ç”¨æˆ·ç»Ÿè®¡
        if user_col and user_col in completion_df.columns:
            try:
                user_type_stats = completion_df.groupby([user_col, type_col]).size().unstack(fill_value=0)
                result["by_user_statistics"] = {
                    "total_users": len(user_type_stats),
                    "user_type_matrix": user_type_stats.to_dict(),
                    "user_totals": user_type_stats.sum(axis=1).to_dict(),
                    "most_productive_users": user_type_stats.sum(axis=1).nlargest(5).to_dict()
                }
            except Exception as e:
                result["by_user_statistics"] = {"error": f"ç”¨æˆ·ç»Ÿè®¡å¤±è´¥: {e}"}
        else:
            result["by_user_statistics"] = {"error": "æœªæ‰¾åˆ°ç”¨æˆ·ä¿¡æ¯"}
        
        return result
    
    def analyze_completion_time(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æ2: å®Œæˆä¸€ä¸ªç±»å‹çš„æ ‡æ³¨éœ€è¦çš„æ—¶é—´"""
        if df.empty:
            return {"error": "æ²¡æœ‰æ•°æ®å¯åˆ†æ"}
        
        # å¯»æ‰¾æ—¶é•¿ç›¸å…³å­—æ®µ
        duration_fields = ['duration', 'completion_time', 'time_taken', 'value']
        duration_col = None
        
        for field in duration_fields:
            if field in df.columns and pd.api.types.is_numeric_dtype(df[field]):
                duration_col = field
                break
        
        # å¯»æ‰¾æ ‡æ³¨ç±»å‹å­—æ®µ
        type_fields = ['annotationType', 'annotation_type', 'toolType', 'tool_type', 'field']
        type_col = None
        
        for field in type_fields:
            if field in df.columns:
                type_col = field
                break
        
        if not duration_col:
            return {"error": "æœªæ‰¾åˆ°æ—¶é•¿æ•°æ®"}
        
        if not type_col:
            return {"error": "æœªæ‰¾åˆ°æ ‡æ³¨ç±»å‹ä¿¡æ¯"}
        
        # è¿‡æ»¤æœ‰æ•ˆçš„å®Œæˆäº‹ä»¶
        valid_mask = (
            (df[duration_col].notna()) & 
            (df[duration_col] > 0) &
            (
                (df.get('action', pd.Series(dtype='object')).fillna('') == 'complete') |
                (df.get('event_type', pd.Series(dtype='object')).fillna('').str.contains('completion|completed', case=False, na=False)) |
                (df.get('field', pd.Series(dtype='object')).fillna('').str.contains('duration|completion', case=False, na=False))
            )
        )
        
        valid_df = df[valid_mask]
        
        if valid_df.empty:
            # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„å®Œæˆæ ‡è¯†ï¼Œä½¿ç”¨æ‰€æœ‰æœ‰durationçš„æ•°æ®
            valid_df = df[(df[duration_col].notna()) & (df[duration_col] > 0)]
        
        if valid_df.empty:
            return {"error": "æ²¡æœ‰æœ‰æ•ˆçš„æ—¶é•¿æ•°æ®"}
        
        # æŒ‰ç±»å‹åˆ†ç»„åˆ†ææ—¶é•¿
        time_stats_by_type = {}
        
        for annotation_type in valid_df[type_col].dropna().unique():
            type_data = valid_df[valid_df[type_col] == annotation_type][duration_col]
            
            if len(type_data) > 0:
                time_stats_by_type[str(annotation_type)] = {
                    "count": len(type_data),
                    "avg_time": float(type_data.mean()),
                    "median_time": float(type_data.median()),
                    "min_time": float(type_data.min()),
                    "max_time": float(type_data.max()),
                    "std_time": float(type_data.std()) if len(type_data) > 1 else 0,
                    "total_time": float(type_data.sum()),
                    "fast_annotations": int((type_data < type_data.median()).sum()),
                    "slow_annotations": int((type_data > type_data.mean() + type_data.std()).sum())
                }
        
        # æ€»ä½“ç»Ÿè®¡
        all_durations = valid_df[duration_col]
        
        # æ•ˆç‡æ´å¯Ÿ
        efficiency_insights = {}
        if time_stats_by_type:
            fastest_type = min(time_stats_by_type.items(), key=lambda x: x[1]['avg_time'])
            slowest_type = max(time_stats_by_type.items(), key=lambda x: x[1]['avg_time'])
            most_consistent_type = min(time_stats_by_type.items(), key=lambda x: x[1]['std_time'])
            
            efficiency_insights = {
                "fastest_type": fastest_type[0],
                "fastest_avg_time": fastest_type[1]['avg_time'],
                "slowest_type": slowest_type[0],
                "slowest_avg_time": slowest_type[1]['avg_time'],
                "most_consistent_type": most_consistent_type[0],
                "most_consistent_std": most_consistent_type[1]['std_time']
            }
        
        return {
            "total_completed_annotations": len(valid_df),
            "overall_completion_time": {
                "avg_time": float(all_durations.mean()),
                "median_time": float(all_durations.median()),
                "min_time": float(all_durations.min()),
                "max_time": float(all_durations.max()),
                "std_time": float(all_durations.std()) if len(all_durations) > 1 else 0
            },
            "by_annotation_type": time_stats_by_type,
            "efficiency_insights": efficiency_insights
        }
    
    def analyze_user_idle_time(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æ3: ç”¨æˆ·æ²¡æœ‰æ“ä½œçš„idleæ—¶é—´"""
        if df.empty:
            return {"error": "æ²¡æœ‰æ•°æ®å¯åˆ†æ"}
        
        # å¯»æ‰¾ç”¨æˆ·å’Œæ—¶é—´å­—æ®µ
        user_fields = ['user_id', 'userId', 'createdBy', 'created_by']
        user_col = None
        
        for field in user_fields:
            if field in df.columns:
                user_col = field
                break
        
        if not user_col or 'time' not in df.columns:
            return {"error": "ç¼ºå°‘ç”¨æˆ·æˆ–æ—¶é—´ä¿¡æ¯"}
        
        # è½¬æ¢æ—¶é—´æ ¼å¼
        df_copy = df.copy()
        df_copy['time'] = pd.to_datetime(df_copy['time'])
        
        # æŒ‰ç”¨æˆ·åˆ†ç»„åˆ†æç©ºé—²æ—¶é—´
        user_idle_stats = {}
        total_idle_time = 0
        idle_periods = []
        
        for user_id in df_copy[user_col].dropna().unique():
            user_data = df_copy[df_copy[user_col] == user_id].sort_values('time')
            
            if len(user_data) < 2:
                continue
            
            # è®¡ç®—ç›¸é‚»äº‹ä»¶ä¹‹é—´çš„æ—¶é—´é—´éš”
            time_diffs = user_data['time'].diff().dropna()  # åˆ é™¤NaTå€¼
            
            # å®šä¹‰ç©ºé—²é˜ˆå€¼
            idle_threshold = pd.Timedelta(minutes=2)  # 2åˆ†é’Ÿæ— æ“ä½œç®—ç©ºé—²
            long_idle_threshold = pd.Timedelta(minutes=10)  # 10åˆ†é’Ÿç®—é•¿æ—¶é—´ç©ºé—²
            
            # ç­›é€‰å‡ºç©ºé—²æ—¶é—´æ®µ
            idle_times = time_diffs[time_diffs > idle_threshold]
            long_idle_times = time_diffs[time_diffs > long_idle_threshold]
            
            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            total_session_time = (user_data['time'].max() - user_data['time'].min()).total_seconds()
            total_user_idle = idle_times.sum().total_seconds() if len(idle_times) > 0 else 0
            
            user_idle_stats[str(user_id)] = {
                "total_events": len(user_data),
                "session_duration": total_session_time,
                "total_idle_time": total_user_idle,
                "idle_periods_count": len(idle_times),
                "long_idle_periods_count": len(long_idle_times),
                "avg_idle_time": total_user_idle / len(idle_times) if len(idle_times) > 0 else 0,
                "max_idle_time": idle_times.max().total_seconds() if len(idle_times) > 0 else 0,
                "idle_ratio": total_user_idle / total_session_time if total_session_time > 0 else 0,
                "activity_level": "é«˜" if total_user_idle / total_session_time < 0.2 else ("ä¸­" if total_user_idle / total_session_time < 0.5 else "ä½")
            }
            
            total_idle_time += total_user_idle
            
            # è®°å½•å…·ä½“çš„ç©ºé—²æ—¶é—´æ®µ
            idle_indices = time_diffs[time_diffs > idle_threshold].index
            for idx in idle_indices:
                if idx > 0:
                    idx_pos = user_data.index.get_loc(idx)
                    if idx_pos > 0:
                        idle_periods.append({
                            "user_id": str(user_id),
                            "idle_duration": time_diffs[idx].total_seconds(),
                            "start_time": str(user_data.iloc[idx_pos-1]['time']),
                            "end_time": str(user_data.iloc[idx_pos]['time'])
                        })
        
        # æ’åºæ‰¾å‡ºæœ€ç©ºé—²å’Œæœ€æ´»è·ƒçš„ç”¨æˆ·
        sorted_users = sorted(user_idle_stats.items(), key=lambda x: x[1]['idle_ratio'], reverse=True)
        
        return {
            "total_users_analyzed": len(user_idle_stats),
            "overall_idle_statistics": {
                "total_idle_time": total_idle_time,
                "avg_idle_ratio": sum(stats['idle_ratio'] for stats in user_idle_stats.values()) / len(user_idle_stats) if user_idle_stats else 0,
                "total_idle_periods": sum(stats['idle_periods_count'] for stats in user_idle_stats.values()),
                "avg_idle_period_duration": total_idle_time / sum(stats['idle_periods_count'] for stats in user_idle_stats.values()) if sum(stats['idle_periods_count'] for stats in user_idle_stats.values()) > 0 else 0
            },
            "by_user_idle_stats": user_idle_stats,
            "idle_insights": {
                "most_idle_user": sorted_users[0][0] if sorted_users else None,
                "most_active_user": sorted_users[-1][0] if sorted_users else None,
                "users_with_high_activity": [user for user, stats in user_idle_stats.items() if stats['activity_level'] == 'é«˜'],
                "users_with_long_idle": [user for user, stats in user_idle_stats.items() if stats['long_idle_periods_count'] > 0]
            },
            "detailed_idle_periods": sorted(idle_periods, key=lambda x: x['idle_duration'], reverse=True)[:10]
        }
    
    def generate_effm_report(self, analysis: Dict[str, Any], hours: int = 1) -> str:
        """ç”ŸæˆEFFMåˆ†ææŠ¥å‘Š"""
        report = f"""
{'='*80}
ğŸ¯ EFFMæ•ˆç‡ç›‘æ§åˆ†ææŠ¥å‘Š
ğŸ“… æ—¶é—´èŒƒå›´: æœ€è¿‘ {hours} å°æ—¶
ğŸ• ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
{'='*80}

ğŸ“Š 1. æ ‡æ³¨ç±»å‹æ•°é‡ç»Ÿè®¡
{'â”€'*50}"""
        
        annotation_stats = analysis.get('annotation_statistics', {})
        if 'overall_statistics' in annotation_stats:
            report += f"\næ€»æ ‡æ³¨æ•°: {annotation_stats.get('total_annotations', 0)}"
            report += f"\næ ‡æ³¨ç±»å‹æ•°: {annotation_stats.get('annotation_types', 0)}\n"
            
            for ann_type, count in annotation_stats['overall_statistics']['by_type'].items():
                percentage = annotation_stats['overall_statistics']['type_percentages'].get(ann_type, 0)
                report += f"{ann_type}: {count} æ¬¡ ({percentage:.1f}%)\n"
            
            # ç”¨æˆ·ç»Ÿè®¡
            user_stats = annotation_stats.get('by_user_statistics', {})
            if 'total_users' in user_stats:
                report += f"\nğŸ‘¥ æ´»è·ƒç”¨æˆ·æ•°: {user_stats['total_users']}"
                report += "\nğŸ† æœ€é«˜äº§ç”¨æˆ·:"
                for user, count in list(user_stats.get('most_productive_users', {}).items())[:3]:
                    report += f"\n  - {user}: {count} æ¬¡æ ‡æ³¨"
        
        report += f"""

â±ï¸ 2. æ ‡æ³¨å®Œæˆæ—¶é—´åˆ†æ
{'â”€'*50}"""
        
        completion_time = analysis.get('annotation_completion_time', {})
        if 'overall_completion_time' in completion_time:
            overall = completion_time['overall_completion_time']
            report += f"\næ€»å®Œæˆæ ‡æ³¨æ•°: {completion_time.get('total_completed_annotations', 0)}"
            report += f"\nå¹³å‡å®Œæˆæ—¶é—´: {overall['avg_time']:.2f} ç§’"
            report += f"\nä¸­ä½æ•°å®Œæˆæ—¶é—´: {overall['median_time']:.2f} ç§’"
            report += f"\næœ€å¿«å®Œæˆæ—¶é—´: {overall['min_time']:.2f} ç§’"
            report += f"\næœ€æ…¢å®Œæˆæ—¶é—´: {overall['max_time']:.2f} ç§’\n"
            
            # æŒ‰ç±»å‹çš„æ—¶é—´ç»Ÿè®¡
            by_type = completion_time.get('by_annotation_type', {})
            for ann_type, stats in by_type.items():
                report += f"\n{ann_type}:"
                report += f"\n  - æ•°é‡: {stats['count']}"
                report += f"\n  - å¹³å‡æ—¶é—´: {stats['avg_time']:.2f} ç§’"
                report += f"\n  - ä¸­ä½æ•°: {stats['median_time']:.2f} ç§’"
            
            # æ•ˆç‡æ´å¯Ÿ
            insights = completion_time.get('efficiency_insights', {})
            if insights:
                report += f"\n\nğŸ’¡ æ•ˆç‡æ´å¯Ÿ:"
                report += f"\næœ€å¿«ç±»å‹: {insights.get('fastest_type', 'N/A')} ({insights.get('fastest_avg_time', 0):.2f}ç§’)"
                report += f"\næœ€æ…¢ç±»å‹: {insights.get('slowest_type', 'N/A')} ({insights.get('slowest_avg_time', 0):.2f}ç§’)"
                report += f"\næœ€ç¨³å®šç±»å‹: {insights.get('most_consistent_type', 'N/A')} (æ ‡å‡†å·®: {insights.get('most_consistent_std', 0):.2f})"
        
        report += f"""

ğŸ˜´ 3. ç”¨æˆ·ç©ºé—²æ—¶é—´åˆ†æ
{'â”€'*50}"""
        
        idle_time = analysis.get('user_idle_time', {})
        if 'overall_idle_statistics' in idle_time:
            overall_idle = idle_time['overall_idle_statistics']
            report += f"\nåˆ†æç”¨æˆ·æ•°: {idle_time.get('total_users_analyzed', 0)}"
            report += f"\næ€»ç©ºé—²æ—¶é—´: {overall_idle['total_idle_time']:.0f} ç§’ ({overall_idle['total_idle_time']/3600:.1f} å°æ—¶)"
            report += f"\nå¹³å‡ç©ºé—²æ¯”ä¾‹: {overall_idle['avg_idle_ratio']*100:.1f}%"
            report += f"\nç©ºé—²æ—¶é—´æ®µæ•°: {overall_idle['total_idle_periods']}"
            report += f"\nå¹³å‡ç©ºé—²æ—¶é•¿: {overall_idle['avg_idle_period_duration']:.0f} ç§’\n"
            
            # ç”¨æˆ·æ´å¯Ÿ
            insights = idle_time.get('idle_insights', {})
            if insights:
                report += f"\nğŸ‘¤ ç”¨æˆ·æ´»è·ƒåº¦åˆ†æ:"
                report += f"\né«˜æ´»è·ƒåº¦ç”¨æˆ·: {len(insights.get('users_with_high_activity', []))} äºº"
                report += f"\næœ‰é•¿æ—¶é—´ç©ºé—²ç”¨æˆ·: {len(insights.get('users_with_long_idle', []))} äºº"
                if insights.get('most_idle_user'):
                    report += f"\næœ€ç©ºé—²ç”¨æˆ·: {insights['most_idle_user']}"
                if insights.get('most_active_user'):
                    report += f"\næœ€æ´»è·ƒç”¨æˆ·: {insights['most_active_user']}"
        
        report += f"\n\n{'='*80}\nğŸ“‹ æŠ¥å‘Šç»“æŸ\n{'='*80}"
        
        return report
    
    def run_analysis(self, hours: int = 1) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„EFFMåˆ†æ"""
        # æŸ¥è¯¢æ•°æ®
        records = self.query_effm_data(hours)
        
        if not records:
            return {"error": "æ— æ³•è·å–æ•°æ®"}
        
        if len(records) == 0:
            return {"error": f"æœ€è¿‘ {hours} å°æ—¶å†…æ²¡æœ‰EFFMæ•°æ®"}
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(records)
        
        print("ğŸ” æ­£åœ¨è¿›è¡ŒEFFMæ•°æ®åˆ†æ...")
        
        # æ‰§è¡Œä¸‰ä¸ªæ ¸å¿ƒåˆ†æ
        analysis = {
            "annotation_statistics": self.analyze_annotation_statistics(df),
            "annotation_completion_time": self.analyze_completion_time(df),
            "user_idle_time": self.analyze_user_idle_time(df),
            "data_overview": {
                "total_records": len(records),
                "time_range": {
                    "start": df['time'].min() if 'time' in df.columns else None,
                    "end": df['time'].max() if 'time' in df.columns else None
                },
                "available_fields": list(df.columns)
            }
        }
        
        return analysis
    
    def close(self):
        """å…³é—­è¿æ¥"""
        if self.client:
            self.client.close()

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='EFFMæ•ˆç‡ç›‘æ§æ•°æ®ä¸“ç”¨åˆ†æå·¥å…·')
    parser.add_argument('--hours', type=int, default=1, help='åˆ†ææœ€è¿‘å¤šå°‘å°æ—¶çš„æ•°æ® (é»˜è®¤: 1)')
    parser.add_argument('--output', type=str, help='è¾“å‡ºæŠ¥å‘Šåˆ°æ–‡ä»¶')
    parser.add_argument('--json', action='store_true', help='è¾“å‡º JSON æ ¼å¼çš„åˆ†æç»“æœ')
    
    args = parser.parse_args()
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = EffmAnalyzer()
    
    try:
        # è¿æ¥æ•°æ®åº“
        if not analyzer.connect():
            sys.exit(1)
        
        # è¿è¡Œåˆ†æ
        analysis = analyzer.run_analysis(args.hours)
        
        if "error" in analysis:
            print(f"âŒ åˆ†æå¤±è´¥: {analysis['error']}")
            sys.exit(1)
        
        if args.json:
            # è¾“å‡º JSON æ ¼å¼
            result = json.dumps(analysis, indent=2, default=str, ensure_ascii=False)
        else:
            # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
            result = analyzer.generate_effm_report(analysis, args.hours)
        
        # è¾“å‡ºç»“æœ
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(result)
            print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")
        else:
            print(result)
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)
    finally:
        analyzer.close()

if __name__ == "__main__":
    main() 