#!/usr/bin/env python3
"""
EFFM 2D Instanceä¸“ç”¨åˆ†æè„šæœ¬
ä¸“é—¨é’ˆå¯¹2D instanceç±»å‹çš„è¯¦ç»†åˆ†æï¼š
- cuboid (çŸ©å½¢/è¾¹ç•Œæ¡†)
- polyline (æŠ˜çº¿)  
- polygon (å¤šè¾¹å½¢)
- issinstance (è¯­ä¹‰åˆ†å‰²å®ä¾‹)

æä¾›ä¸ªäººå’Œæ€»ä½“çš„è¯¦ç»†æ•ˆç‡ç»Ÿè®¡
"""

import sys
import json
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import argparse
import numpy as np

try:
    from influxdb_client.client.influxdb_client import InfluxDBClient
    from influxdb_client.client.query_api import QueryApi
except ImportError:
    print("âŒ ç¼ºå°‘ influxdb-client ä¾èµ–")
    print("è¯·å®‰è£…: pip install influxdb-client pandas numpy")
    sys.exit(1)

# InfluxDB é…ç½®
INFLUXDB_CONFIG = {
    'url': 'http://localhost:8087',
    'token': 'Y7anaf-f1yBZaDe3M1pCy5LdfVTxH8g8odTzf0UOJd_0V4BROJmQ7HlFDTLefh8GIoWNNkOgKHdnKAeR7KMhqw==',
    'org': 'xtreme1',
    'bucket': 'efficiency_events'
}

# 2D Instanceç±»å‹æ˜ å°„
INSTANCE_TYPE_MAPPING = {
    # åŸºç¡€ç±»å‹æ˜ å°„
    'rect': 'cuboid',
    'rectangle': 'cuboid', 
    'bounding_box': 'cuboid',
    'BOUNDING_BOX': 'cuboid',
    'RECTANGLE': 'cuboid',
    
    # å¤šè¾¹å½¢ç±»å‹
    'polygon': 'polygon',
    'POLYGON': 'polygon',
    
    # æŠ˜çº¿ç±»å‹
    'polyline': 'polyline',
    'POLYLINE': 'polyline',
    
    # ISSç›¸å…³ç±»å‹
    'iss': 'issinstance',
    'iss-rect': 'issinstance',
    'iss_unified': 'issinstance',
    'ISS': 'issinstance',
    'ISS_UNIFIED': 'issinstance',
    'ISS_RECT': 'issinstance',
    
    # å…³é”®ç‚¹ï¼ˆè™½ç„¶ä¸æ˜¯2D instanceï¼Œä½†åŒ…å«è¿›æ¥ï¼‰
    'keypoint': 'keypoint',
    'key-point': 'keypoint',
    'KEY_POINT': 'keypoint',
}

class Effm2DInstanceAnalyzer:
    def __init__(self):
        """åˆå§‹åŒ– 2D Instance åˆ†æå™¨"""
        self.client = None
        self.query_api = None
        
    def connect(self) -> bool:
        """è¿æ¥åˆ° InfluxDB"""
        try:
            self.client = InfluxDBClient(
                url=INFLUXDB_CONFIG['url'],
                token=INFLUXDB_CONFIG['token'],
                org=INFLUXDB_CONFIG['org']
            )
            self.query_api = self.client.query_api()
            
            # æµ‹è¯•è¿æ¥
            health = self.client.health()
            if health.status == "pass":
                print("âœ… InfluxDB è¿æ¥æˆåŠŸ")
                return True
            else:
                print(f"âŒ InfluxDB å¥åº·æ£€æŸ¥å¤±è´¥: {health.status}")
                return False
                
        except Exception as e:
            print(f"âŒ InfluxDB è¿æ¥å¤±è´¥: {e}")
            return False
    
    def query_2d_instance_data(self, hours: int = 1) -> Optional[List[Dict]]:
        """æŸ¥è¯¢2D Instanceç›¸å…³çš„EFFMäº‹ä»¶æ•°æ®"""
        try:
            query = f'''
            from(bucket: "{INFLUXDB_CONFIG['bucket']}")
              |> range(start: -{hours}h)
              |> filter(fn: (r) => r["_measurement"] == "efficiency_events")
              |> filter(fn: (r) => 
                r["annotationType"] == "rect" or
                r["annotationType"] == "rectangle" or 
                r["annotationType"] == "bounding_box" or
                r["annotationType"] == "cuboid" or
                r["annotationType"] == "polygon" or
                r["annotationType"] == "polyline" or
                r["meta_toolType"] == "rect" or
                r["meta_toolType"] == "rectangle" or
                r["meta_toolType"] == "polygon" or
                r["meta_toolType"] == "polyline" or
                r["meta_toolType"] == "iss" or
                r["toolType"] == "rect" or
                r["toolType"] == "rectangle" or
                r["toolType"] == "polygon" or
                r["toolType"] == "polyline" or
                r["toolType"] == "iss" or
                r["field"] == "rect" or 
                r["field"] == "polygon" or 
                r["field"] == "polyline" or 
                r["field"] == "iss"
              )
              |> sort(columns: ["_time"], desc: false)
            '''
            
            print(f"ğŸ” æ­£åœ¨æŸ¥è¯¢æœ€è¿‘ {hours} å°æ—¶çš„2D Instanceæ•°æ®...")
            
            if not self.query_api:
                raise Exception("Query API not initialized")
            
            result = self.query_api.query(org=INFLUXDB_CONFIG['org'], query=query)
            
            records = []
            for table in result:
                for record in table.records:
                    records.append({
                        'time': record.get_time(),
                        'measurement': record.get_measurement(),
                        'field': record.get_field(),
                        'value': record.get_value(),
                        **record.values
                    })
            
            print(f"âœ… è·å–åˆ° {len(records)} æ¡2D Instanceè®°å½•")
            return records
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢2D Instanceæ•°æ®å¤±è´¥: {e}")
            return None
    
    def normalize_instance_type(self, tool_type: str) -> str:
        """æ ‡å‡†åŒ–å®ä¾‹ç±»å‹åç§°"""
        if not tool_type:
            return 'unknown'
        
        tool_type_clean = str(tool_type).strip().lower()
        return INSTANCE_TYPE_MAPPING.get(tool_type_clean, tool_type_clean)
    
    def analyze_2d_instance_statistics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æ2D Instanceç±»å‹ç»Ÿè®¡ï¼ˆæ€»ä½“å’Œä¸ªäººï¼‰"""
        if df.empty:
            return {"error": "æ²¡æœ‰æ•°æ®å¯åˆ†æ"}
        
        # æ·»åŠ æ ‡å‡†åŒ–çš„å®ä¾‹ç±»å‹å­—æ®µ - åŒ…å«å®é™…æ•°æ®ä¸­çš„å­—æ®µ
        df['normalized_instance_type'] = df.apply(
            lambda row: self.normalize_instance_type(
                row.get('annotationType') or 
                row.get('meta_toolType') or 
                row.get('toolType') or 
                row.get('tool_type') or 
                row.get('field', '')
            ), axis=1
        )
        
        # è¿‡æ»¤å‡º2D instanceç›¸å…³çš„æ•°æ®
        instance_types = ['cuboid', 'polygon', 'polyline', 'issinstance']
        instance_df = df[df['normalized_instance_type'].isin(instance_types)]
        
        if instance_df.empty:
            return {"error": "æ²¡æœ‰æ‰¾åˆ°2D Instanceç›¸å…³æ•°æ®"}
        
        # è¿‡æ»¤å®Œæˆçš„æ ‡æ³¨äº‹ä»¶
        completion_mask = (
            (instance_df.get('action', pd.Series(dtype='object')).fillna('') == 'complete') |
            (instance_df.get('event_type', pd.Series(dtype='object')).fillna('').str.contains('completion|completed', case=False, na=False)) |
            (instance_df.get('field', pd.Series(dtype='object')).fillna('').str.contains('completion|completed', case=False, na=False))
        )
        
        completion_df = instance_df[completion_mask]
        
        if completion_df.empty:
            completion_df = instance_df  # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„å®Œæˆæ ‡è¯†ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®
        
        # æ€»ä½“ç»Ÿè®¡
        total_by_type = completion_df['normalized_instance_type'].value_counts()
        
        result = {
            "total_2d_instances": len(completion_df),
            "instance_types_count": len(total_by_type),
            "overall_statistics": {
                "by_type": total_by_type.to_dict(),
                "type_percentages": (total_by_type / len(completion_df) * 100).to_dict(),
                "type_ranking": total_by_type.index.tolist()
            }
        }
        
        # å¯»æ‰¾ç”¨æˆ·å­—æ®µ
        user_fields = ['user_id', 'userId', 'createdBy', 'created_by']
        user_col = None
        for field in user_fields:
            if field in completion_df.columns:
                user_col = field
                break
        
        # åˆ†ç”¨æˆ·ç»Ÿè®¡
        if user_col and user_col in completion_df.columns:
            try:
                user_type_stats = completion_df.groupby([user_col, 'normalized_instance_type']).size().unstack(fill_value=0)
                
                # è®¡ç®—æ¯ä¸ªç”¨æˆ·çš„ç»Ÿè®¡ä¿¡æ¯
                user_detailed_stats = {}
                for user_id in user_type_stats.index:
                    user_data = user_type_stats.loc[user_id]
                    total_user_instances = user_data.sum()
                    
                    user_detailed_stats[str(user_id)] = {
                        "total_instances": int(total_user_instances),
                        "by_type": user_data.to_dict(),
                        "type_percentages": (user_data / total_user_instances * 100).to_dict() if total_user_instances > 0 else {},
                        "most_used_type": user_data.idxmax() if total_user_instances > 0 else None,
                        "most_used_count": int(user_data.max()) if total_user_instances > 0 else 0,
                        "diversity_score": len(user_data[user_data > 0])  # ä½¿ç”¨äº†å¤šå°‘ç§ä¸åŒç±»å‹
                    }
                
                result["individual_statistics"] = {
                    "total_users": len(user_type_stats),
                    "user_details": user_detailed_stats,
                    "user_totals": user_type_stats.sum(axis=1).to_dict(),
                    "most_productive_users": user_type_stats.sum(axis=1).nlargest(5).to_dict(),
                    "type_specialists": self._find_type_specialists(user_type_stats),
                    "diversity_ranking": self._calculate_user_diversity(user_detailed_stats)
                }
            except Exception as e:
                result["individual_statistics"] = {"error": f"ç”¨æˆ·ç»Ÿè®¡å¤±è´¥: {e}"}
        else:
            result["individual_statistics"] = {"error": "æœªæ‰¾åˆ°ç”¨æˆ·ä¿¡æ¯"}
        
        return result
    
    def analyze_2d_instance_efficiency(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æ2D Instanceæ ‡æ³¨æ•ˆç‡ï¼ˆæ€»ä½“å’Œä¸ªäººï¼‰"""
        if df.empty:
            return {"error": "æ²¡æœ‰æ•°æ®å¯åˆ†æ"}
        
        # æ ‡å‡†åŒ–å®ä¾‹ç±»å‹ - åŒ…å«å®é™…æ•°æ®ä¸­çš„å­—æ®µ
        df['normalized_instance_type'] = df.apply(
            lambda row: self.normalize_instance_type(
                row.get('annotationType') or 
                row.get('meta_toolType') or 
                row.get('toolType') or 
                row.get('tool_type') or 
                row.get('field', '')
            ), axis=1
        )
        
        # è¿‡æ»¤2D instanceæ•°æ®
        instance_types = ['cuboid', 'polygon', 'polyline', 'issinstance']
        instance_df = df[df['normalized_instance_type'].isin(instance_types)]
        
        if instance_df.empty:
            return {"error": "æ²¡æœ‰æ‰¾åˆ°2D Instanceç›¸å…³æ•°æ®"}
        
        # å¯»æ‰¾æ—¶é•¿ç›¸å…³å­—æ®µ - ä½¿ç”¨å®é™…æ•°æ®ä¸­çš„å­—æ®µ
        duration_fields = ['meta_averagePointInterval', 'meta_duration', 'duration', 'completion_time', 'time_taken', 'value']
        duration_col = None
        
        for field in duration_fields:
            if field in instance_df.columns:
                # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ï¼ˆå¤„ç†å­—ç¬¦ä¸²æ•°å€¼ï¼‰
                try:
                    numeric_values = pd.to_numeric(instance_df[field], errors='coerce').dropna()
                    if len(numeric_values) > 0:
                        if field == 'meta_averagePointInterval':
                            # meta_averagePointInterval é€šå¸¸åœ¨ 100-50000 æ¯«ç§’èŒƒå›´
                            if numeric_values.min() > 0 and numeric_values.max() < 100000:
                                duration_col = field
                                break
                        elif field != 'duration':  # æ’é™¤å¼‚å¸¸å¤§çš„ duration å­—æ®µ
                            # å…¶ä»–å­—æ®µåº”è¯¥åœ¨åˆç†èŒƒå›´å†…
                            if numeric_values.min() > 0 and numeric_values.max() < 3600000:  # å°äº1å°æ—¶
                                duration_col = field
                                break
                except Exception:
                    continue
        
        if not duration_col:
            return {"error": f"æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ—¶é•¿æ•°æ®ã€‚å¯ç”¨å­—æ®µ: {list(instance_df.columns)}"}
        
        # è½¬æ¢ duration_col ä¸ºæ•°å€¼å‹
        instance_df[f'{duration_col}_numeric'] = pd.to_numeric(instance_df[duration_col], errors='coerce')
        
        # è¿‡æ»¤æœ‰æ•ˆçš„å®Œæˆäº‹ä»¶ - ä½¿ç”¨å®é™…æ•°æ®ç»“æ„
        valid_mask = (
            (instance_df[f'{duration_col}_numeric'].notna()) & 
            (instance_df[f'{duration_col}_numeric'] > 0) &
            (
                (instance_df.get('type', pd.Series(dtype='object')).fillna('') == 'complete') |
                (instance_df.get('action', pd.Series(dtype='object')).fillna('') == 'complete') |
                (instance_df.get('event_type', pd.Series(dtype='object')).fillna('').str.contains('completion|completed', case=False, na=False)) |
                (instance_df.get('field', pd.Series(dtype='object')).fillna('').str.contains('duration|completion', case=False, na=False))
            )
        )
        
        valid_df = instance_df[valid_mask]
        
        if valid_df.empty:
            # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„å®Œæˆæ ‡è¯†ï¼Œä½¿ç”¨æ‰€æœ‰æœ‰durationçš„æ•°æ®
            valid_df = instance_df[(instance_df[f'{duration_col}_numeric'].notna()) & (instance_df[f'{duration_col}_numeric'] > 0)]
        
        if valid_df.empty:
            return {"error": "æ²¡æœ‰æœ‰æ•ˆçš„æ—¶é•¿æ•°æ®"}
        
        # ä½¿ç”¨æ•°å€¼å‹åˆ—åè¿›è¡Œåˆ†æ
        numeric_duration_col = f'{duration_col}_numeric'
        
        # æ€»ä½“æ•ˆç‡åˆ†æ
        overall_efficiency = self._analyze_overall_efficiency(valid_df, numeric_duration_col)
        
        # æŒ‰ç±»å‹çš„æ•ˆç‡åˆ†æ
        type_efficiency = self._analyze_efficiency_by_type(valid_df, numeric_duration_col)
        
        # æŒ‰ç”¨æˆ·çš„æ•ˆç‡åˆ†æ
        user_efficiency = self._analyze_efficiency_by_user(valid_df, numeric_duration_col)
        
        return {
            "total_completed_instances": len(valid_df),
            "overall_efficiency": overall_efficiency,
            "efficiency_by_type": type_efficiency,
            "efficiency_by_user": user_efficiency,
            "efficiency_insights": self._generate_efficiency_insights(type_efficiency, user_efficiency)
        }
    
    def analyze_2d_instance_idle_patterns(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æ2D Instanceæ ‡æ³¨è¿‡ç¨‹ä¸­çš„ç©ºé—²æ¨¡å¼ï¼ˆæ€»ä½“å’Œä¸ªäººï¼‰"""
        if df.empty:
            return {"error": "æ²¡æœ‰æ•°æ®å¯åˆ†æ"}
        
        # æ ‡å‡†åŒ–å®ä¾‹ç±»å‹ - åŒ…å«å®é™…æ•°æ®ä¸­çš„å­—æ®µ
        df['normalized_instance_type'] = df.apply(
            lambda row: self.normalize_instance_type(
                row.get('annotationType') or 
                row.get('meta_toolType') or 
                row.get('toolType') or 
                row.get('tool_type') or 
                row.get('field', '')
            ), axis=1
        )
        
        # è¿‡æ»¤2D instanceæ•°æ®
        instance_types = ['cuboid', 'polygon', 'polyline', 'issinstance']
        instance_df = df[df['normalized_instance_type'].isin(instance_types)]
        
        if instance_df.empty:
            return {"error": "æ²¡æœ‰æ‰¾åˆ°2D Instanceç›¸å…³æ•°æ®"}
        
        # å¯»æ‰¾ç”¨æˆ·å’Œæ—¶é—´å­—æ®µ
        user_fields = ['user_id', 'userId', 'createdBy', 'created_by']
        user_col = None
        
        for field in user_fields:
            if field in instance_df.columns:
                user_col = field
                break
        
        if not user_col or 'time' not in instance_df.columns:
            return {"error": "ç¼ºå°‘ç”¨æˆ·æˆ–æ—¶é—´ä¿¡æ¯"}
        
        # è½¬æ¢æ—¶é—´æ ¼å¼
        df_copy = instance_df.copy()
        df_copy['time'] = pd.to_datetime(df_copy['time'])
        
        # æ€»ä½“ç©ºé—²åˆ†æ
        overall_idle = self._analyze_overall_idle_patterns(df_copy, user_col)
        
        # æŒ‰å®ä¾‹ç±»å‹çš„ç©ºé—²åˆ†æ
        type_idle = self._analyze_idle_by_instance_type(df_copy, user_col)
        
        # æŒ‰ç”¨æˆ·çš„ç©ºé—²åˆ†æ
        user_idle = self._analyze_idle_by_user(df_copy, user_col)
        
        return {
            "total_users_analyzed": len(df_copy[user_col].dropna().unique()),
            "overall_idle_patterns": overall_idle,
            "idle_by_instance_type": type_idle,
            "idle_by_user": user_idle,
            "idle_insights": self._generate_idle_insights(type_idle, user_idle)
        }
    
    def _find_type_specialists(self, user_type_stats: pd.DataFrame) -> Dict[str, Any]:
        """æ‰¾å‡ºå„ä¸ªç±»å‹çš„ä¸“å®¶ç”¨æˆ·"""
        specialists = {}
        
        for instance_type in user_type_stats.columns:
            if instance_type in user_type_stats.columns:
                type_data = user_type_stats[instance_type]
                if type_data.sum() > 0:
                    top_user = type_data.idxmax()
                    specialists[instance_type] = {
                        "user": str(top_user),
                        "count": int(type_data.max()),
                        "percentage": float(type_data.max() / type_data.sum() * 100)
                    }
        
        return specialists
    
    def _calculate_user_diversity(self, user_detailed_stats: Dict) -> List[Dict]:
        """è®¡ç®—ç”¨æˆ·çš„å¤šæ ·æ€§æ’å"""
        diversity_list = []
        
        for user_id, stats in user_detailed_stats.items():
            diversity_list.append({
                "user": user_id,
                "diversity_score": stats["diversity_score"],
                "total_instances": stats["total_instances"]
            })
        
        # æŒ‰å¤šæ ·æ€§åˆ†æ•°æ’åº
        diversity_list.sort(key=lambda x: x["diversity_score"], reverse=True)
        return diversity_list[:10]  # è¿”å›å‰10å
    
    def _analyze_overall_efficiency(self, df: pd.DataFrame, duration_col: str) -> Dict[str, Any]:
        """åˆ†ææ€»ä½“æ•ˆç‡"""
        durations = df[duration_col]
        
        return {
            "avg_completion_time": float(durations.mean()),
            "median_completion_time": float(durations.median()),
            "min_completion_time": float(durations.min()),
            "max_completion_time": float(durations.max()),
            "std_completion_time": float(durations.std()) if len(durations) > 1 else 0,
            "efficiency_distribution": {
                "fast_annotations": int((durations < durations.quantile(0.25)).sum()),
                "normal_annotations": int(((durations >= durations.quantile(0.25)) & (durations <= durations.quantile(0.75))).sum()),
                "slow_annotations": int((durations > durations.quantile(0.75)).sum())
            }
        }
    
    def _analyze_efficiency_by_type(self, df: pd.DataFrame, duration_col: str) -> Dict[str, Any]:
        """æŒ‰å®ä¾‹ç±»å‹åˆ†ææ•ˆç‡"""
        type_efficiency = {}
        
        for instance_type in df['normalized_instance_type'].dropna().unique():
            type_data = df[df['normalized_instance_type'] == instance_type][duration_col]
            
            if len(type_data) > 0:
                type_efficiency[str(instance_type)] = {
                    "count": len(type_data),
                    "avg_time": float(type_data.mean()),
                    "median_time": float(type_data.median()),
                    "min_time": float(type_data.min()),
                    "max_time": float(type_data.max()),
                    "std_time": float(type_data.std()) if len(type_data) > 1 else 0,
                    "efficiency_score": self._calculate_efficiency_score(type_data),
                    "consistency_score": self._calculate_consistency_score(type_data)
                }
        
        return type_efficiency
    
    def _analyze_efficiency_by_user(self, df: pd.DataFrame, duration_col: str) -> Dict[str, Any]:
        """æŒ‰ç”¨æˆ·åˆ†ææ•ˆç‡"""
        user_fields = ['user_id', 'userId', 'createdBy', 'created_by']
        user_col = None
        
        for field in user_fields:
            if field in df.columns:
                user_col = field
                break
        
        if not user_col:
            return {"error": "æœªæ‰¾åˆ°ç”¨æˆ·ä¿¡æ¯"}
        
        user_efficiency = {}
        
        for user_id in df[user_col].dropna().unique():
            user_data = df[df[user_col] == user_id]
            user_durations = user_data[duration_col]
            
            if len(user_durations) > 0:
                # æŒ‰ç±»å‹çš„æ•ˆç‡
                type_breakdown = {}
                for instance_type in user_data['normalized_instance_type'].dropna().unique():
                    type_durations = user_data[user_data['normalized_instance_type'] == instance_type][duration_col]
                    if len(type_durations) > 0:
                        type_breakdown[str(instance_type)] = {
                            "count": len(type_durations),
                            "avg_time": float(type_durations.mean()),
                            "min_time": float(type_durations.min()),
                            "max_time": float(type_durations.max())
                        }
                
                user_efficiency[str(user_id)] = {
                    "total_instances": len(user_durations),
                    "overall_avg_time": float(user_durations.mean()),
                    "overall_median_time": float(user_durations.median()),
                    "efficiency_score": self._calculate_efficiency_score(user_durations),
                    "consistency_score": self._calculate_consistency_score(user_durations),
                    "by_instance_type": type_breakdown,
                    "improvement_trend": self._calculate_improvement_trend(user_data, duration_col)
                }
        
        return user_efficiency
    
    def _analyze_overall_idle_patterns(self, df: pd.DataFrame, user_col: str) -> Dict[str, Any]:
        """åˆ†ææ€»ä½“ç©ºé—²æ¨¡å¼"""
        total_idle_time = 0
        total_idle_periods = 0
        all_idle_durations = []
        
        for user_id in df[user_col].dropna().unique():
            user_data = df[df[user_col] == user_id].sort_values('time')
            
            if len(user_data) < 2:
                continue
            
            time_diffs = user_data['time'].diff().dropna()
            idle_threshold = pd.Timedelta(minutes=2)
            idle_times = time_diffs[time_diffs > idle_threshold]
            
            if len(idle_times) > 0:
                total_idle_time += idle_times.sum().total_seconds()
                total_idle_periods += len(idle_times)
                all_idle_durations.extend([t.total_seconds() for t in idle_times])
        
        return {
            "total_idle_time": total_idle_time,
            "total_idle_periods": total_idle_periods,
            "avg_idle_duration": total_idle_time / total_idle_periods if total_idle_periods > 0 else 0,
            "median_idle_duration": float(np.median(all_idle_durations)) if all_idle_durations else 0,
            "max_idle_duration": max(all_idle_durations) if all_idle_durations else 0
        }
    
    def _analyze_idle_by_instance_type(self, df: pd.DataFrame, user_col: str) -> Dict[str, Any]:
        """æŒ‰å®ä¾‹ç±»å‹åˆ†æç©ºé—²æ¨¡å¼"""
        type_idle = {}
        
        for instance_type in df['normalized_instance_type'].dropna().unique():
            type_df = df[df['normalized_instance_type'] == instance_type]
            idle_data = []
            
            for user_id in type_df[user_col].dropna().unique():
                user_data = type_df[type_df[user_col] == user_id].sort_values('time')
                
                if len(user_data) < 2:
                    continue
                
                time_diffs = user_data['time'].diff().dropna()
                idle_threshold = pd.Timedelta(minutes=2)
                idle_times = time_diffs[time_diffs > idle_threshold]
                
                if len(idle_times) > 0:
                    idle_data.extend([t.total_seconds() for t in idle_times])
            
            if idle_data:
                type_idle[str(instance_type)] = {
                    "idle_periods_count": len(idle_data),
                    "avg_idle_duration": float(np.mean(idle_data)),
                    "median_idle_duration": float(np.median(idle_data)),
                    "max_idle_duration": max(idle_data)
                }
        
        return type_idle
    
    def _analyze_idle_by_user(self, df: pd.DataFrame, user_col: str) -> Dict[str, Any]:
        """æŒ‰ç”¨æˆ·åˆ†æç©ºé—²æ¨¡å¼"""
        user_idle = {}
        
        for user_id in df[user_col].dropna().unique():
            user_data = df[df[user_col] == user_id].sort_values('time')
            
            if len(user_data) < 2:
                continue
            
            time_diffs = user_data['time'].diff().dropna()
            idle_threshold = pd.Timedelta(minutes=2)
            long_idle_threshold = pd.Timedelta(minutes=10)
            
            idle_times = time_diffs[time_diffs > idle_threshold]
            long_idle_times = time_diffs[time_diffs > long_idle_threshold]
            
            total_session_time = (user_data['time'].max() - user_data['time'].min()).total_seconds()
            total_user_idle = idle_times.sum().total_seconds() if len(idle_times) > 0 else 0
            
            user_idle[str(user_id)] = {
                "total_instances": len(user_data),
                "session_duration": total_session_time,
                "total_idle_time": total_user_idle,
                "idle_periods_count": len(idle_times),
                "long_idle_periods_count": len(long_idle_times),
                "idle_ratio": total_user_idle / total_session_time if total_session_time > 0 else 0,
                "avg_idle_duration": total_user_idle / len(idle_times) if len(idle_times) > 0 else 0,
                "max_idle_duration": idle_times.max().total_seconds() if len(idle_times) > 0 else 0
            }
        
        return user_idle
    
    def _calculate_efficiency_score(self, durations: pd.Series) -> float:
        """è®¡ç®—æ•ˆç‡åˆ†æ•°ï¼ˆ0-100ï¼Œåˆ†æ•°è¶Šé«˜è¶Šé«˜æ•ˆï¼‰"""
        if len(durations) == 0:
            return 0.0
        
        # åŸºäºæ—¶é—´çš„æ•ˆç‡åˆ†æ•°ï¼šå¿«é€Ÿå®Œæˆè·å¾—é«˜åˆ†
        median_time = durations.median()
        avg_time = durations.mean()
        
        # å¦‚æœå¹³å‡æ—¶é—´æ¥è¿‘ä¸­ä½æ•°ï¼Œè¯´æ˜ç¨³å®šæ€§å¥½
        stability_factor = 1 - abs(avg_time - median_time) / avg_time if avg_time > 0 else 0
        
        # åŸºç¡€æ•ˆç‡åˆ†æ•°ï¼ˆåæ¯”äºå¹³å‡æ—¶é—´ï¼‰
        base_score = max(0, 100 - (avg_time / 10))  # å‡è®¾10ç§’ä¸ºåŸºå‡†
        
        # ç»“åˆç¨³å®šæ€§
        efficiency_score = base_score * (0.7 + 0.3 * stability_factor)
        
        return min(100.0, max(0.0, efficiency_score))
    
    def _calculate_consistency_score(self, durations: pd.Series) -> float:
        """è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°ï¼ˆ0-100ï¼Œåˆ†æ•°è¶Šé«˜è¶Šä¸€è‡´ï¼‰"""
        if len(durations) <= 1:
            return 100.0
        
        # åŸºäºå˜å¼‚ç³»æ•°çš„ä¸€è‡´æ€§åˆ†æ•°
        cv = durations.std() / durations.mean() if durations.mean() > 0 else float('inf')
        
        # å˜å¼‚ç³»æ•°è¶Šå°ï¼Œä¸€è‡´æ€§è¶Šé«˜
        consistency_score = max(0, 100 - cv * 50)
        
        return min(100.0, max(0.0, consistency_score))
    
    def _calculate_improvement_trend(self, user_data: pd.DataFrame, duration_col: str) -> str:
        """è®¡ç®—ç”¨æˆ·æ”¹è¿›è¶‹åŠ¿"""
        if len(user_data) < 5:  # éœ€è¦è¶³å¤Ÿçš„æ•°æ®ç‚¹
            return "insufficient_data"
        
        # æŒ‰æ—¶é—´æ’åºï¼Œè®¡ç®—æ—¶é—´è¶‹åŠ¿
        sorted_data = user_data.sort_values('time')
        durations = sorted_data[duration_col].values
        
        # ç®€å•çº¿æ€§è¶‹åŠ¿ï¼šæ¯”è¾ƒå‰åŠéƒ¨åˆ†å’ŒååŠéƒ¨åˆ†çš„å¹³å‡å€¼
        mid_point = len(durations) // 2
        first_half_avg = np.mean(durations[:mid_point])
        second_half_avg = np.mean(durations[mid_point:])
        
        improvement_ratio = (first_half_avg - second_half_avg) / first_half_avg if first_half_avg > 0 else 0
        
        if improvement_ratio > 0.1:
            return "improving"
        elif improvement_ratio < -0.1:
            return "declining"
        else:
            return "stable"
    
    def _generate_efficiency_insights(self, type_efficiency: Dict, user_efficiency: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆæ•ˆç‡æ´å¯Ÿ"""
        insights = {}
        
        if type_efficiency:
            # æœ€é«˜æ•ˆçš„å®ä¾‹ç±»å‹
            fastest_type = min(type_efficiency.items(), key=lambda x: x[1]['avg_time'])
            slowest_type = max(type_efficiency.items(), key=lambda x: x[1]['avg_time'])
            most_consistent_type = max(type_efficiency.items(), key=lambda x: x[1]['consistency_score'])
            
            insights["type_insights"] = {
                "fastest_type": {
                    "type": fastest_type[0],
                    "avg_time": fastest_type[1]['avg_time']
                },
                "slowest_type": {
                    "type": slowest_type[0],
                    "avg_time": slowest_type[1]['avg_time']
                },
                "most_consistent_type": {
                    "type": most_consistent_type[0],
                    "consistency_score": most_consistent_type[1]['consistency_score']
                }
            }
        
        if user_efficiency:
            # æœ€é«˜æ•ˆçš„ç”¨æˆ·
            most_efficient_user = max(user_efficiency.items(), key=lambda x: x[1]['efficiency_score'])
            most_consistent_user = max(user_efficiency.items(), key=lambda x: x[1]['consistency_score'])
            
            # æ”¹è¿›è¶‹åŠ¿ç»Ÿè®¡
            improvement_stats = {}
            for trend in ['improving', 'stable', 'declining']:
                count = sum(1 for user_data in user_efficiency.values() 
                           if user_data['improvement_trend'] == trend)
                improvement_stats[trend] = count
            
            insights["user_insights"] = {
                "most_efficient_user": {
                    "user": most_efficient_user[0],
                    "efficiency_score": most_efficient_user[1]['efficiency_score']
                },
                "most_consistent_user": {
                    "user": most_consistent_user[0],
                    "consistency_score": most_consistent_user[1]['consistency_score']
                },
                "improvement_trends": improvement_stats
            }
        
        return insights
    
    def _generate_idle_insights(self, type_idle: Dict, user_idle: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆç©ºé—²æ—¶é—´æ´å¯Ÿ"""
        insights = {}
        
        if type_idle:
            # æ‰¾å‡ºç©ºé—²æ—¶é—´æœ€å¤šçš„å®ä¾‹ç±»å‹
            highest_idle_type = max(type_idle.items(), key=lambda x: x[1]['avg_idle_duration'])
            insights["type_insights"] = {
                "highest_idle_type": {
                    "type": highest_idle_type[0],
                    "avg_idle_duration": highest_idle_type[1]['avg_idle_duration']
                }
            }
        
        if user_idle:
            # æ‰¾å‡ºæœ€æ´»è·ƒå’Œæœ€ç©ºé—²çš„ç”¨æˆ·
            most_active_user = min(user_idle.items(), key=lambda x: x[1]['idle_ratio'])
            most_idle_user = max(user_idle.items(), key=lambda x: x[1]['idle_ratio'])
            
            # æ´»è·ƒåº¦åˆ†ç±»
            activity_levels = {"high": 0, "medium": 0, "low": 0}
            for user_data in user_idle.values():
                idle_ratio = user_data['idle_ratio']
                if idle_ratio < 0.2:
                    activity_levels["high"] += 1
                elif idle_ratio < 0.5:
                    activity_levels["medium"] += 1
                else:
                    activity_levels["low"] += 1
            
            insights["user_insights"] = {
                "most_active_user": {
                    "user": most_active_user[0],
                    "idle_ratio": most_active_user[1]['idle_ratio']
                },
                "most_idle_user": {
                    "user": most_idle_user[0],
                    "idle_ratio": most_idle_user[1]['idle_ratio']
                },
                "activity_distribution": activity_levels
            }
        
        return insights
    
    def generate_2d_instance_report(self, analysis: Dict[str, Any], hours: int = 1) -> str:
        """ç”Ÿæˆ2D Instanceä¸“ç”¨åˆ†ææŠ¥å‘Š"""
        report = f"""
{'='*90}
ğŸ¯ 2D Instance æ•ˆç‡ç›‘æ§ä¸“ç”¨åˆ†ææŠ¥å‘Š
ğŸ“… æ—¶é—´èŒƒå›´: æœ€è¿‘ {hours} å°æ—¶
ğŸ• ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
{'='*90}

ğŸ—ï¸ 1. 2D Instance ç±»å‹ç»Ÿè®¡åˆ†æ
{'â”€'*60}"""
        
        stats = analysis.get('instance_statistics', {})
        if 'overall_statistics' in stats:
            report += f"\næ€»2D Instanceæ•°: {stats.get('total_2d_instances', 0)}"
            report += f"\nå®ä¾‹ç±»å‹æ•°: {stats.get('instance_types_count', 0)}\n"
            
            overall = stats['overall_statistics']
            for instance_type, count in overall['by_type'].items():
                percentage = overall['type_percentages'].get(instance_type, 0)
                report += f"{instance_type}: {count} ä¸ª ({percentage:.1f}%)\n"
            
            # ä¸ªäººç»Ÿè®¡
            individual = stats.get('individual_statistics', {})
            if 'total_users' in individual:
                report += f"\nğŸ‘¥ æ´»è·ƒç”¨æˆ·æ•°: {individual['total_users']}"
                report += "\nğŸ† æœ€é«˜äº§ç”¨æˆ·:"
                for user, count in list(individual.get('most_productive_users', {}).items())[:3]:
                    report += f"\n  - {user}: {count} ä¸ªå®ä¾‹"
                
                # ç±»å‹ä¸“å®¶
                specialists = individual.get('type_specialists', {})
                if specialists:
                    report += "\n\nğŸ¯ ç±»å‹ä¸“å®¶:"
                    for inst_type, specialist in specialists.items():
                        report += f"\n  - {inst_type}: {specialist['user']} ({specialist['count']}ä¸ª, {specialist['percentage']:.1f}%)"
                
                # å¤šæ ·æ€§æ’å
                diversity = individual.get('diversity_ranking', [])
                if diversity:
                    report += "\n\nğŸŒˆ å¤šæ ·æ€§æ’å (ä½¿ç”¨ç±»å‹æœ€å¤šçš„ç”¨æˆ·):"
                    for i, user_info in enumerate(diversity[:3], 1):
                        report += f"\n  {i}. {user_info['user']}: {user_info['diversity_score']} ç§ç±»å‹"
        
        report += f"""

âš¡ 2. 2D Instance æ•ˆç‡åˆ†æ
{'â”€'*60}"""
        
        efficiency = analysis.get('instance_efficiency', {})
        if 'overall_efficiency' in efficiency:
            overall_eff = efficiency['overall_efficiency']
            report += f"\næ€»å®Œæˆå®ä¾‹æ•°: {efficiency.get('total_completed_instances', 0)}"
            report += f"\nå¹³å‡å®Œæˆæ—¶é—´: {overall_eff['avg_completion_time']:.2f} ç§’"
            report += f"\nä¸­ä½æ•°å®Œæˆæ—¶é—´: {overall_eff['median_completion_time']:.2f} ç§’"
            report += f"\næœ€å¿«å®Œæˆæ—¶é—´: {overall_eff['min_completion_time']:.2f} ç§’"
            report += f"\næœ€æ…¢å®Œæˆæ—¶é—´: {overall_eff['max_completion_time']:.2f} ç§’\n"
            
            # æ•ˆç‡åˆ†å¸ƒ
            dist = overall_eff.get('efficiency_distribution', {})
            report += f"\nğŸ“Š æ•ˆç‡åˆ†å¸ƒ:"
            report += f"\nå¿«é€Ÿæ ‡æ³¨: {dist.get('fast_annotations', 0)} ä¸ª"
            report += f"\næ­£å¸¸æ ‡æ³¨: {dist.get('normal_annotations', 0)} ä¸ª"
            report += f"\næ…¢é€Ÿæ ‡æ³¨: {dist.get('slow_annotations', 0)} ä¸ª"
            
            # æŒ‰ç±»å‹çš„æ•ˆç‡
            by_type = efficiency.get('efficiency_by_type', {})
            if by_type:
                report += f"\n\nğŸ” å„å®ä¾‹ç±»å‹æ•ˆç‡:"
                for inst_type, eff_data in by_type.items():
                    report += f"\n{inst_type}:"
                    report += f"\n  - æ•°é‡: {eff_data['count']}"
                    report += f"\n  - å¹³å‡æ—¶é—´: {eff_data['avg_time']:.2f} ç§’"
                    report += f"\n  - æ•ˆç‡åˆ†æ•°: {eff_data['efficiency_score']:.1f}/100"
                    report += f"\n  - ä¸€è‡´æ€§åˆ†æ•°: {eff_data['consistency_score']:.1f}/100"
            
            # æ•ˆç‡æ´å¯Ÿ
            insights = efficiency.get('efficiency_insights', {})
            if insights:
                type_insights = insights.get('type_insights', {})
                if type_insights:
                    report += f"\n\nğŸ’¡ ç±»å‹æ•ˆç‡æ´å¯Ÿ:"
                    report += f"\næœ€å¿«ç±»å‹: {type_insights.get('fastest_type', {}).get('type', 'N/A')} ({type_insights.get('fastest_type', {}).get('avg_time', 0):.2f}ç§’)"
                    report += f"\næœ€æ…¢ç±»å‹: {type_insights.get('slowest_type', {}).get('type', 'N/A')} ({type_insights.get('slowest_type', {}).get('avg_time', 0):.2f}ç§’)"
                    report += f"\næœ€ç¨³å®šç±»å‹: {type_insights.get('most_consistent_type', {}).get('type', 'N/A')} (ä¸€è‡´æ€§: {type_insights.get('most_consistent_type', {}).get('consistency_score', 0):.1f})"
                
                user_insights = insights.get('user_insights', {})
                if user_insights:
                    report += f"\n\nğŸ‘¤ ç”¨æˆ·æ•ˆç‡æ´å¯Ÿ:"
                    report += f"\næœ€é«˜æ•ˆç”¨æˆ·: {user_insights.get('most_efficient_user', {}).get('user', 'N/A')} (æ•ˆç‡åˆ†æ•°: {user_insights.get('most_efficient_user', {}).get('efficiency_score', 0):.1f})"
                    report += f"\næœ€ç¨³å®šç”¨æˆ·: {user_insights.get('most_consistent_user', {}).get('user', 'N/A')} (ä¸€è‡´æ€§åˆ†æ•°: {user_insights.get('most_consistent_user', {}).get('consistency_score', 0):.1f})"
                    
                    trends = user_insights.get('improvement_trends', {})
                    report += f"\næ”¹è¿›è¶‹åŠ¿: æå‡ä¸­({trends.get('improving', 0)}äºº) | ç¨³å®š({trends.get('stable', 0)}äºº) | ä¸‹é™ä¸­({trends.get('declining', 0)}äºº)"
        
        report += f"""

ğŸ˜´ 3. 2D Instance ç©ºé—²æ—¶é—´åˆ†æ
{'â”€'*60}"""
        
        idle = analysis.get('instance_idle_patterns', {})
        if 'overall_idle_patterns' in idle:
            overall_idle = idle['overall_idle_patterns']
            report += f"\nåˆ†æç”¨æˆ·æ•°: {idle.get('total_users_analyzed', 0)}"
            report += f"\næ€»ç©ºé—²æ—¶é—´: {overall_idle['total_idle_time']:.0f} ç§’ ({overall_idle['total_idle_time']/3600:.1f} å°æ—¶)"
            report += f"\nç©ºé—²æ—¶é—´æ®µæ•°: {overall_idle['total_idle_periods']}"
            report += f"\nå¹³å‡ç©ºé—²æ—¶é•¿: {overall_idle['avg_idle_duration']:.0f} ç§’"
            report += f"\næœ€é•¿ç©ºé—²æ—¶é—´: {overall_idle['max_idle_duration']:.0f} ç§’\n"
            
            # æŒ‰ç±»å‹çš„ç©ºé—²åˆ†æ
            by_type_idle = idle.get('idle_by_instance_type', {})
            if by_type_idle:
                report += f"\nğŸ” å„å®ä¾‹ç±»å‹ç©ºé—²æƒ…å†µ:"
                for inst_type, idle_data in by_type_idle.items():
                    report += f"\n{inst_type}: å¹³å‡ç©ºé—² {idle_data['avg_idle_duration']:.0f}ç§’ ({idle_data['idle_periods_count']}æ¬¡)"
            
            # ç©ºé—²æ´å¯Ÿ
            idle_insights = idle.get('idle_insights', {})
            if idle_insights:
                type_insights = idle_insights.get('type_insights', {})
                if type_insights:
                    highest_idle = type_insights.get('highest_idle_type', {})
                    report += f"\n\nğŸ’¤ ç©ºé—²æ´å¯Ÿ:"
                    report += f"\nç©ºé—²æœ€å¤šçš„ç±»å‹: {highest_idle.get('type', 'N/A')} (å¹³å‡ {highest_idle.get('avg_idle_duration', 0):.0f}ç§’)"
                
                user_insights = idle_insights.get('user_insights', {})
                if user_insights:
                    report += f"\næœ€æ´»è·ƒç”¨æˆ·: {user_insights.get('most_active_user', {}).get('user', 'N/A')} (ç©ºé—²æ¯”ä¾‹: {user_insights.get('most_active_user', {}).get('idle_ratio', 0)*100:.1f}%)"
                    report += f"\næœ€ç©ºé—²ç”¨æˆ·: {user_insights.get('most_idle_user', {}).get('user', 'N/A')} (ç©ºé—²æ¯”ä¾‹: {user_insights.get('most_idle_user', {}).get('idle_ratio', 0)*100:.1f}%)"
                    
                    activity_dist = user_insights.get('activity_distribution', {})
                    report += f"\næ´»è·ƒåº¦åˆ†å¸ƒ: é«˜æ´»è·ƒ({activity_dist.get('high', 0)}äºº) | ä¸­æ´»è·ƒ({activity_dist.get('medium', 0)}äºº) | ä½æ´»è·ƒ({activity_dist.get('low', 0)}äºº)"
        
        report += f"\n\n{'='*90}\nğŸ“‹ 2D Instance ä¸“ç”¨æŠ¥å‘Šç»“æŸ\n{'='*90}"
        
        return report
    
    def run_2d_instance_analysis(self, hours: int = 1) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„2D Instanceåˆ†æ"""
        # æŸ¥è¯¢æ•°æ®
        records = self.query_2d_instance_data(hours)
        
        if not records:
            return {"error": "æ— æ³•è·å–æ•°æ®"}
        
        if len(records) == 0:
            return {"error": f"æœ€è¿‘ {hours} å°æ—¶å†…æ²¡æœ‰2D Instanceæ•°æ®"}
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(records)
        
        print("ğŸ” æ­£åœ¨è¿›è¡Œ2D Instanceä¸“ç”¨åˆ†æ...")
        
        # æ‰§è¡Œä¸‰ä¸ªæ ¸å¿ƒåˆ†æ
        analysis = {
            "instance_statistics": self.analyze_2d_instance_statistics(df),
            "instance_efficiency": self.analyze_2d_instance_efficiency(df),
            "instance_idle_patterns": self.analyze_2d_instance_idle_patterns(df),
            "data_overview": {
                "total_records": len(records),
                "time_range": {
                    "start": df['time'].min() if 'time' in df.columns else None,
                    "end": df['time'].max() if 'time' in df.columns else None
                },
                "available_fields": list(df.columns),
                "instance_type_mapping": INSTANCE_TYPE_MAPPING
            }
        }
        
        return analysis
    
    def close(self):
        """å…³é—­è¿æ¥"""
        if self.client:
            self.client.close()

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='2D Instanceæ•ˆç‡ç›‘æ§ä¸“ç”¨åˆ†æå·¥å…·')
    parser.add_argument('--hours', type=int, default=1, help='åˆ†ææœ€è¿‘å¤šå°‘å°æ—¶çš„æ•°æ® (é»˜è®¤: 1)')
    parser.add_argument('--output', type=str, help='è¾“å‡ºæŠ¥å‘Šåˆ°æ–‡ä»¶')
    parser.add_argument('--json', action='store_true', help='è¾“å‡º JSON æ ¼å¼çš„åˆ†æç»“æœ')
    
    args = parser.parse_args()
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = Effm2DInstanceAnalyzer()
    
    try:
        # è¿æ¥æ•°æ®åº“
        if not analyzer.connect():
            sys.exit(1)
        
        # è¿è¡Œåˆ†æ
        analysis = analyzer.run_2d_instance_analysis(args.hours)
        
        if "error" in analysis:
            print(f"âŒ åˆ†æå¤±è´¥: {analysis['error']}")
            sys.exit(1)
        
        if args.json:
            # è¾“å‡º JSON æ ¼å¼
            result = json.dumps(analysis, indent=2, default=str, ensure_ascii=False)
        else:
            # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
            result = analyzer.generate_2d_instance_report(analysis, args.hours)
        
        # è¾“å‡ºç»“æœ
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(result)
            print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")
        else:
            print(result)
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)
    finally:
        analyzer.close()

if __name__ == "__main__":
    main() 